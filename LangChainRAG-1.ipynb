{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613e9ae7-60e6-492c-8f58-959eae39551f",
   "metadata": {},
   "source": [
    "## Install Requirements\n",
    "Run the cell, below, to install the libraries we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b586e-c781-49fc-a60e-bc13ce4e78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain langchain-core langchain_community langchain_text_splitters langgraph \n",
    "# !pip install -qU langchain-google-genai \n",
    "# !pip install -qU bs4 \n",
    "# !pip install -qU python-dotenv typing_extensions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372d5e7-db78-4e70-abcb-527ed3172fb5",
   "metadata": {},
   "source": [
    "## Load the API key into the environment\n",
    "The code, below, loads the API key and stores it where the LangChain libraries (and likely the Google libraries used by the LangChain libraries) expect to find it.\n",
    "\n",
    "**If you're running this code in Google Colab**, this code assumes you've already stored your API key as a *secret*:\n",
    "\n",
    "1. Open your Google Colab notebook and click on the ðŸ”‘ Secrets tab in the left panel.\n",
    "2. The Secrets tab is found on the left panel.\n",
    "3. Create a new secret with the name `GOOGLE_API_KEY`.\n",
    "4. Copy/paste your API key into the Value input box of `GOOGLE_API_KEY`.\n",
    "5. Toggle the button on the left to allow notebook access to the secret.\n",
    "\n",
    "Otherwise, the code assumes that you have a `.env` file that includes `GOOGLE_API_KEY=<your api key here>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e14fed-2868-4890-b286-36cb11a77551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "API_KEY = 'GOOGLE_API_KEY'\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import userdata\n",
    "    os.environ[API_KEY] = userdata.get(API_KEY)\n",
    "    os.environ[API_KEY]    \n",
    "else:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # Load environment variables from .env file; should include GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1eea1-5beb-4ea6-bdb2-e58b77b43364",
   "metadata": {},
   "source": [
    "You can verify that your API key is where it ought to be by uncommenting and running the code cell, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2eff26-1a8b-4df4-9148-faeb91c64fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getenv(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be63fe-dfa6-43f6-847d-89ce013c82a6",
   "metadata": {},
   "source": [
    "## Components\n",
    "Import and instantiate a:\n",
    "  1. chat model\n",
    "  2. embedding model\n",
    "  3. in-memory vector store\n",
    "\n",
    "Note that we're using the `langchain_google_genai` library instead of the Google Vertex (or OpenAI, or Anthropic, etc.) library. That means you can't simply copy code from the LangChain tutorial. Documentation for the Google GenAI library can be found [here](https://python.langchain.com/api_reference/google_genai/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f543d-f139-49a6-a445-a15a0d6e5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite-preview-02-05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf1193-4665-48bd-83e1-c7ffa58fbd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203071a-d2a5-4c04-b4c8-6400a1b1dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cf55a-d311-43da-bce6-438e70b9ca26",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ed668-0ced-44cb-afe4-61f9e6f23d90",
   "metadata": {},
   "source": [
    "### Scrape a Web Page\n",
    "\n",
    "We'll use the `WebBaseLoader` class to scrape a web page we'd like to ask an LLM about. It uses [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) -- another popular library -- to parse the web page (extract its text content). \n",
    "\n",
    "Notice how, instead of writing their own HTML parser, the LangChain developers make use of another well-established library. The named parameter `bs_kwargs` is short for \"Beautiful Soup key-word arguments. We're passing to `WebBaseLoader` a set of arguments that will be passed to Beautiful Soup functions. A decision to use another library like this comes with trade-offs:\n",
    "  - To use LangChain, I don't have to write much or any code to control Beautiful Soup. LangChain handles (almost) all of it for me.\n",
    "  - But now this LangChain class is dependent on (tied to) Beautiful Soup. If Beautiful Soup changes interfaces, `WebBaseLoader` might break.\n",
    "  - And `WebBaseLoader` is also somewhat less flexible. What if Beautiful Soup isn't my prefered library or doesn't do what I need? So you'll sometimes see one library give you the ability to pass whatever HTML parser you choose. It could be Beautiful Soup or another open-source library or the HTML parser you wrote for fun.\n",
    "\n",
    "Notice also that we've decided to give Beautiful Soup some more specific instructions, taking content from HTML tags that have a class of `post-content`, `post-title`, or `post-header`. (You could navigate to the web page and open the developer tools to see just what that includes.) Doing so gives us cleaner text to use for our RAG application but at the cost of making our code less general. If I want to query a different web page, there's no reason to think it will use the same class names to identify the important bits. If we add a web page loader to KnotebookLM, we'll need to think about how best to generalize our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc34ac-f19c-40e0-a10e-f4712b208f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# setting a User-Agent to avoid a Beautiful Soup warning\n",
    "# a User-Agent header tells a web server what kind of client is making the request \n",
    "os.environ['USER_AGENT'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443664c2-7f19-42ec-b78e-cc05d6c0005c",
   "metadata": {},
   "source": [
    "`docs` is a list of `Document` objects. We only loaded one document, so the length of `docs` is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e6729-d8a7-4fb1-81a7-eb2c61c211ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1398b08c-5ce1-41a0-8ea4-da57c5183b98",
   "metadata": {},
   "source": [
    "We can ask Python to tell us the type of that sole document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52be27e-69bf-40b8-9950-c353492c3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb85ca-d2d2-4ba3-937c-e755dd436bc3",
   "metadata": {},
   "source": [
    "That's `langchain-core`'s base `Document` class. Consulting the [documentation](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) you can see it is instantiated with two notable properties: `page_content` and `metadata`. (You can also find in the documentation a link to the source code if you want to dig further.)\n",
    "\n",
    "We'll need to talk about `metadata` later. For now, let's look at the first bit of the `page_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9db16-c120-4605-8ab0-bf9b01e3e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bc8f2-38cf-4435-bf9e-eae4aa7fcf12",
   "metadata": {},
   "source": [
    "Compare it to the web page we scraped. Beautiful Soup did a pretty good job, no?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979cb17-906f-4f88-8e6a-4a1dda941b70",
   "metadata": {},
   "source": [
    "### Split the Text\n",
    "As a final pre-processing step, we'll split the text into smaller chunks. Read [why](https://python.langchain.com/docs/concepts/text_splitters/#why-split-documents).\n",
    "\n",
    "Following the tutorial, we'll use the `RecursiveCharacterTextSplitter` class. It implements a [text-structure based](https://python.langchain.com/docs/concepts/text_splitters/#text-structured-based) approach. To better understand how this splitter works and how to control it, read this [guide](https://python.langchain.com/docs/how_to/recursive_text_splitter/) and consult the [documentation](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef62a9f-fd2b-4a9a-8053-3beec03d663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10418d2a-e88b-4928-b9bb-362465c61ccf",
   "metadata": {},
   "source": [
    "Let's see how many chunks we've split our document into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba6483-5741-455f-82e6-85702913123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f270c46-f04d-420e-9d4f-ca0946d902f7",
   "metadata": {},
   "source": [
    "They're not all equal length. Based on the guides and documentation you've read, can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323611eb-b7ce-4a51-86a2-12b98527d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, split in enumerate(all_splits[:5]):\n",
    "    print(f\"Split {idx} length: {len(split.page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ee6fb-70f6-4ce7-8c4d-308a5bbeafc2",
   "metadata": {},
   "source": [
    "Based on what we read, we'd expect to see some overlap between the end of one split and the beginning of the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54720d22-28ec-4ccb-981d-1c0f4f0bbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prev, curr in zip(all_splits[20:25], all_splits[21:26]):\n",
    "    print('previous: \\n', prev.page_content[-50:], '\\n')\n",
    "    print('current: \\n', curr.page_content[:50], '\\n')\n",
    "    print('\\n------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c392ee3-77d3-4c6f-a76a-bcf0bfb9a8bf",
   "metadata": {},
   "source": [
    "But in this slice of splits, I don't see any overlap. (I tried a few different slices and likewise didn't see any overlaps.) Does that mean it's not working? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90f378-4273-4527-bde4-ced79b586318",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(all_splits) - 1):\n",
    "    last = all_splits[i].page_content.strip().split()[-1]\n",
    "    first = all_splits[i+1].page_content.strip().split()[0]\n",
    "    if last == first:\n",
    "        print('\\n-------- index', i, '----------\\n')\n",
    "        print('previous: \\n', last, '\\n')\n",
    "        print('current: \\n', first, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93ae62-278a-4ead-9089-49aae6098663",
   "metadata": {},
   "source": [
    "There are only two cases where one split overlaps with the previous, and in both cases it looks like a heading. It's not perfectly clear -- at least not to me -- why `RecursiveCharacterTextSplitter` works this way. It could be the nature of the web page (lots of headings, lots of figures). It could be our the relation between our `chunk_size` and `chunk_overlap`. The documentation isn't super helpful. If we want to know more, we'll likely have to dive into the code and experiment.\n",
    "\n",
    "When it comes time to write code for KnotebookLM, we'll likely want to play around with chunk and overlap sizes and see what makes most sense for our application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20980be4-adaa-4630-860f-b69dcd236804",
   "metadata": {},
   "source": [
    "### Index Splits\n",
    "\n",
    "If you were implementing this next step without LangChain, you'd likely think of it as two steps:\n",
    "1. For each split, generate an embedding (a vector that represents the \"meaning\" of the text in the split)\n",
    "2. Write the resulting vector and the original text to a database.\n",
    "\n",
    "LangChain handles both with a single call to the `add_documents` method on the `vector_store` instance we created. (And now you understand why we needed to pass the `embeddings` instance as an argument to `vector_store`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b14fc-1154-4690-8548-62aaf61e06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2d870-7e4c-4763-9872-bc5da8ddb48b",
   "metadata": {},
   "source": [
    "That's all the pre-processing we need. We're ready to move on to retrieval tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d2856-3dac-40ee-8cd3-3c836d0b72d4",
   "metadata": {},
   "source": [
    "## Retrieve Relevant Chunks, Ask Questions\n",
    "\n",
    "We've indexed the web page and can now ask questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c9354-9ca6-4d99-b033-2cbcf6ecec2c",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "\n",
    "LangChain has a library of task-specific prompts. Let's grab the \"RAG\" prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f690343-da3f-41f7-9d22-484517157900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt');\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f61ce-7b1a-4f89-aba0-3435885a6dc7",
   "metadata": {},
   "source": [
    "Notice it didn't return a simple string, but rather an instance of `ChatPromptTemplate`. Following the tutorial's walk-through, we can explore it a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d03ce4-73dc-4cc1-9c21-ffb132c82808",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_message, = prompt.invoke(\n",
    "    { \"context\": \"Here's where we'll put relevant chunks from the web page.\", \"question\": \"Here's where our question goes.\" }\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b550908-7101-444b-96b0-81dcb651898e",
   "metadata": {},
   "source": [
    "Notice the comma after `example_message`? That wasn't a mistake. As `to_messages` implies, we might get more than one message. Adding the comma there *destructures* the list `to_messages` returns so that I get just the first item. (In this case, there is only one item.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02ae57-0e11-4f70-af1e-619dc719de6c",
   "metadata": {},
   "source": [
    "Let's see the `content` of that message..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d120448-9c8e-428a-bf5e-e2d69480d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f404ced-ca53-495c-98f9-108350a02193",
   "metadata": {},
   "source": [
    "Pretty cool. We pass the prompt a dictionary with `context` and `question` keys and it'll insert their values into our prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06478a0c-43bb-4ae0-9414-4ca90c720279",
   "metadata": {},
   "source": [
    "### Using LangGraph to Stitch Together the Parts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
