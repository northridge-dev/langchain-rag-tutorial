{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613e9ae7-60e6-492c-8f58-959eae39551f",
   "metadata": {},
   "source": [
    "## Install Requirements\n",
    "Run the cell, below, to install the libraries we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b586e-c781-49fc-a60e-bc13ce4e78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain langchain-core langchain_community langchain_text_splitters langgraph \n",
    "# !pip install -qU langchain-google-genai \n",
    "# !pip install -qU bs4 \n",
    "# !pip install -qU python-dotenv typing_extensions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372d5e7-db78-4e70-abcb-527ed3172fb5",
   "metadata": {},
   "source": [
    "## Load the API key into the environment\n",
    "The code, below, loads the API key and stores it where the LangChain libraries (and likely the Google libraries used by the LangChain libraries) expect to find it.\n",
    "\n",
    "**If you're running this code in Google Colab**, this code assumes you've already stored your API key as a *secret*:\n",
    "\n",
    "1. Open your Google Colab notebook and click on the ðŸ”‘ Secrets tab in the left panel.\n",
    "2. The Secrets tab is found on the left panel.\n",
    "3. Create a new secret with the name `GOOGLE_API_KEY`.\n",
    "4. Copy/paste your API key into the Value input box of `GOOGLE_API_KEY`.\n",
    "5. Toggle the button on the left to allow notebook access to the secret.\n",
    "\n",
    "Otherwise, the code assumes that you have a `.env` file that includes `GOOGLE_API_KEY=<your api key here>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e14fed-2868-4890-b286-36cb11a77551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "API_KEY = 'GOOGLE_API_KEY'\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import userdata\n",
    "    os.environ[API_KEY] = userdata.get(API_KEY)\n",
    "    os.environ[API_KEY]    \n",
    "else:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # Load environment variables from .env file; should include GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1eea1-5beb-4ea6-bdb2-e58b77b43364",
   "metadata": {},
   "source": [
    "You can verify that your API key is where it ought to be by uncommenting and running the code cell, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2eff26-1a8b-4df4-9148-faeb91c64fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getenv(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be63fe-dfa6-43f6-847d-89ce013c82a6",
   "metadata": {},
   "source": [
    "## Components\n",
    "Import and instantiate a:\n",
    "  1. chat model\n",
    "  2. embedding model\n",
    "  3. in-memory vector store\n",
    "\n",
    "Note that we're using the `langchain_google_genai` library instead of the Google Vertex (or OpenAI, or Anthropic, etc.) library. That means you can't simply copy code from the LangChain tutorial. Documentation for the Google GenAI library can be found [here](https://python.langchain.com/api_reference/google_genai/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f543d-f139-49a6-a445-a15a0d6e5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf1193-4665-48bd-83e1-c7ffa58fbd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203071a-d2a5-4c04-b4c8-6400a1b1dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cf55a-d311-43da-bce6-438e70b9ca26",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ed668-0ced-44cb-afe4-61f9e6f23d90",
   "metadata": {},
   "source": [
    "### Scrape a Web Page\n",
    "\n",
    "We'll use the `WebBaseLoader` class to scrape a web page we'd like to ask an LLM about. It uses [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) -- another popular library -- to parse the web page (extract its text content). \n",
    "\n",
    "Notice how, instead of writing their own HTML parser, the LangChain developers make use of another well-established library. The named parameter `bs_kwargs` is short for \"Beautiful Soup key-word arguments. We're passing to `WebBaseLoader` a set of arguments that will be passed to Beautiful Soup functions. A decision to use another library like this comes with trade-offs:\n",
    "  - To use LangChain, I don't have to write much or any code to control Beautiful Soup. LangChain handles (almost) all of it for me.\n",
    "  - But now this LangChain class is dependent on (tied to) Beautiful Soup. If Beautiful Soup changes interfaces, `WebBaseLoader` might break.\n",
    "  - And `WebBaseLoader` is also somewhat less flexible. What if Beautiful Soup isn't my prefered library or doesn't do what I need? So you'll sometimes see one library give you the ability to pass whatever HTML parser you choose. It could be Beautiful Soup or another open-source library or the HTML parser you wrote for fun.\n",
    "\n",
    "Notice also that we've decided to give Beautiful Soup some more specific instructions, taking content from HTML tags that have a class of `post-content`, `post-title`, or `post-header`. (You could navigate to the web page and open the developer tools to see just what that includes.) Doing so gives us cleaner text to use for our RAG application but at the cost of making our code less general. If I want to query a different web page, there's no reason to think it will use the same class names to identify the important bits. If we add a web page loader to KnotebookLM, we'll need to think about how best to generalize our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc34ac-f19c-40e0-a10e-f4712b208f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# setting a User-Agent to avoid a Beautiful Soup warning\n",
    "# a User-Agent header tells a web server what kind of client is making the request \n",
    "os.environ['USER_AGENT'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443664c2-7f19-42ec-b78e-cc05d6c0005c",
   "metadata": {},
   "source": [
    "`docs` is a list of `Document` objects. We only loaded one document, so the length of `docs` is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e6729-d8a7-4fb1-81a7-eb2c61c211ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1398b08c-5ce1-41a0-8ea4-da57c5183b98",
   "metadata": {},
   "source": [
    "We can ask Python to tell us the type of that sole document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52be27e-69bf-40b8-9950-c353492c3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb85ca-d2d2-4ba3-937c-e755dd436bc3",
   "metadata": {},
   "source": [
    "That's `langchain-core`'s base `Document` class. Consulting the [documentation](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) you can see it is instantiated with two notable properties: `page_content` and `metadata`. (You can also find in the documentation a link to the source code if you want to dig further.)\n",
    "\n",
    "We'll need to talk about `metadata` later. For now, let's look at the first bit of the `page_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9db16-c120-4605-8ab0-bf9b01e3e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bc8f2-38cf-4435-bf9e-eae4aa7fcf12",
   "metadata": {},
   "source": [
    "Compare it to the web page we scraped. Beautiful Soup did a pretty good job, no?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979cb17-906f-4f88-8e6a-4a1dda941b70",
   "metadata": {},
   "source": [
    "### Split the Text\n",
    "As a final pre-processing step, we'll split the text into smaller chunks. Read [why](https://python.langchain.com/docs/concepts/text_splitters/#why-split-documents).\n",
    "\n",
    "Following the tutorial, we'll use the `RecursiveCharacterTextSplitter` class. It implements a [text-structure based](https://python.langchain.com/docs/concepts/text_splitters/#text-structured-based) approach. To better understand how this splitter works and how to control it, read this [guide](https://python.langchain.com/docs/how_to/recursive_text_splitter/) and consult the [documentation](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef62a9f-fd2b-4a9a-8053-3beec03d663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10418d2a-e88b-4928-b9bb-362465c61ccf",
   "metadata": {},
   "source": [
    "Let's see how many chunks we've split our document into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba6483-5741-455f-82e6-85702913123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f270c46-f04d-420e-9d4f-ca0946d902f7",
   "metadata": {},
   "source": [
    "They're not all equal length. Based on the guides and documentation you've read, can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323611eb-b7ce-4a51-86a2-12b98527d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, split in enumerate(all_splits[:5]):\n",
    "    print(f\"Split {idx} length: {len(split.page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ee6fb-70f6-4ce7-8c4d-308a5bbeafc2",
   "metadata": {},
   "source": [
    "Based on what we read, we'd expect to see some overlap between the end of one split and the beginning of the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54720d22-28ec-4ccb-981d-1c0f4f0bbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prev, curr in zip(all_splits[20:25], all_splits[21:26]):\n",
    "    print('previous: \\n', prev.page_content[-50:], '\\n')\n",
    "    print('current: \\n', curr.page_content[:50], '\\n')\n",
    "    print('\\n------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c392ee3-77d3-4c6f-a76a-bcf0bfb9a8bf",
   "metadata": {},
   "source": [
    "But in this slice of splits, I don't see any overlap. (I tried a few different slices and likewise didn't see any overlaps.) Does that mean it's not working? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90f378-4273-4527-bde4-ced79b586318",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(all_splits) - 1):\n",
    "    last = all_splits[i].page_content.strip().split()[-1]\n",
    "    first = all_splits[i+1].page_content.strip().split()[0]\n",
    "    if last == first:\n",
    "        print('\\n-------- index', i, '----------\\n')\n",
    "        print('previous: \\n', last, '\\n')\n",
    "        print('current: \\n', first, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93ae62-278a-4ead-9089-49aae6098663",
   "metadata": {},
   "source": [
    "There are only two cases where one split overlaps with the previous, and in both cases it looks like a heading. It's not perfectly clear -- at least not to me -- why `RecursiveCharacterTextSplitter` works this way. It could be the nature of the web page (lots of headings, lots of figures). It could be our the relation between our `chunk_size` and `chunk_overlap`. The documentation isn't super helpful. If we want to know more, we'll likely have to dive into the code and experiment.\n",
    "\n",
    "When it comes time to write code for KnotebookLM, we'll likely want to play around with chunk and overlap sizes and see what makes most sense for our application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20980be4-adaa-4630-860f-b69dcd236804",
   "metadata": {},
   "source": [
    "### Index Splits\n",
    "\n",
    "If you were implementing this next step without LangChain, you'd likely think of it as two steps:\n",
    "1. For each split, generate an embedding (a vector that represents the \"meaning\" of the text in the split)\n",
    "2. Write the resulting vector and the original text to a database.\n",
    "\n",
    "LangChain handles both with a single call to the `add_documents` method on the `vector_store` instance we created. (And now you understand why we needed to pass the `embeddings` instance as an argument to `vector_store`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b14fc-1154-4690-8548-62aaf61e06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a7394-3cd4-4cb4-89b5-1db342868b17",
   "metadata": {},
   "source": [
    "Let's take a quick peak at what's in `vector_store`. Each document has a unique identifier (the `id`), the `text` and `metadata` we saw when exploring the splits, and a `vector` -- the document's embedding. Maybe it's a little hard to tell, but those embeddings were returned by making calls to the Google embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830e5c4-f05d-4646-a0ab-ad106d07aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (id, doc) in enumerate(vector_store.store.items()):\n",
    "    if index < 3:\n",
    "        # docs have keys 'id', 'vector', 'text', 'metadata'\n",
    "        print(f\"{id}: {doc['text'][:75]}\")\n",
    "        print(doc['metadata'])\n",
    "        print(doc['vector'][:5])\n",
    "        print(\"\\n\\n---------------\\n\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2d870-7e4c-4763-9872-bc5da8ddb48b",
   "metadata": {},
   "source": [
    "That's all the pre-processing we need. We're ready to move on to retrieval tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d2856-3dac-40ee-8cd3-3c836d0b72d4",
   "metadata": {},
   "source": [
    "## Retrieve Relevant Chunks, Ask Questions\n",
    "\n",
    "We've indexed the web page and can now ask questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c9354-9ca6-4d99-b033-2cbcf6ecec2c",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "\n",
    "LangChain has a library of task-specific prompts. Let's grab the \"RAG\" prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f690343-da3f-41f7-9d22-484517157900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt');\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f61ce-7b1a-4f89-aba0-3435885a6dc7",
   "metadata": {},
   "source": [
    "Notice it didn't return a simple string, but rather an instance of `ChatPromptTemplate`. Following the tutorial's walk-through, we can explore it a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d03ce4-73dc-4cc1-9c21-ffb132c82808",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_message, = prompt.invoke(\n",
    "    { \"context\": \"Here's where we'll put relevant chunks from the web page.\", \"question\": \"Here's where our question goes.\" }\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b550908-7101-444b-96b0-81dcb651898e",
   "metadata": {},
   "source": [
    "Notice the comma after `example_message`? That wasn't a mistake. As `to_messages` implies, we might get more than one message. Adding the comma there *destructures* the list `to_messages` returns so that I get just the first item. (In this case, there is only one item.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02ae57-0e11-4f70-af1e-619dc719de6c",
   "metadata": {},
   "source": [
    "Let's see the `content` of that message..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d120448-9c8e-428a-bf5e-e2d69480d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f404ced-ca53-495c-98f9-108350a02193",
   "metadata": {},
   "source": [
    "Pretty cool. We pass the prompt a dictionary with `context` and `question` keys and it'll insert their values into our prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06478a0c-43bb-4ae0-9414-4ca90c720279",
   "metadata": {},
   "source": [
    "### Using LangGraph to Stitch Together the Parts\n",
    "Here's how the docs describe LangGraph:\n",
    "> LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows.\n",
    "\n",
    "Translation: LangGraph is a tool for building an application that can coordinate interactions between a user and one or more AI tools -- in this case, our chat and embedding models and the vector store where we stash our document's embeddings. To do so, it needs to \"remember\" (that's what \"stateful\" means).\n",
    "\n",
    "Basically, we'll create the structure of its memory (the `State` class) and define its two operations, `retrieve` and `generate`. Then we define a workflow and \"compile\" it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8588026-ad97-4e97-bfe1-321177cdcee7",
   "metadata": {},
   "source": [
    "We'll start by importing some classes and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5dd31-b6c7-474e-af49-f1888f412aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19577b4-0fd8-4c07-bc1f-232308b5e1eb",
   "metadata": {},
   "source": [
    "#### State\n",
    "Next, we'll create the application `State`. It's a class that will inherit from the `TypedDict` class. \n",
    "\n",
    "We need to remember the question, the context (the relevant chunks of the web page), and the answer. The *types* (e.g., `str`, `List`) may look new. We're defining what kind of data will be stored at each property. Type informaton helps us (and our tools) catch errors and can make tools like autocompletion more effective. (Not Python, but some languages are *typed* -- they **require** type information.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5edb68-0345-48f0-a329-f3b5b3c6b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efc3d2-a99f-434d-a716-7c3b6c321241",
   "metadata": {},
   "source": [
    "#### Retrieval Task\n",
    "Let's now define the `retrieve` task. This is how we get the most relevant chunks from our document. All we have to do is pass our question to the `similarity_search` method exposed by our `vector_store` instance. The rest is abstracted away. `similarity_search` handles:\n",
    "  1. generating embeddings for our question\n",
    "  2. using those embeddings, searching through the vector store for the chunks closest to it in \"meaning space\"\n",
    "  3. return to us the most similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c5c78-dc5a-42be-a8b0-21ba43eef150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4cc228-f08b-454a-b546-2e1b6986303a",
   "metadata": {},
   "source": [
    "We don't have to wait to put together the whole \"graph\" (application). We can test drive `retrieve` ourselves. (Note: to help us later, I'm going to add the result of `retrieve` to `test_state`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf41cd-6e4d-41c5-bb41-dad7472abcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state = State({ \"question\": \"Who wrote the blog post?\" })\n",
    "test_state[\"context\"] = retrieve(test_state)[\"context\"]\n",
    "test_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212d004-3ad9-4137-a484-6d42dcde2477",
   "metadata": {},
   "source": [
    "I got back four chunks, probably ordered by similarity. None of them seem to contain the relevant information. (The second chunk mentions an author -- and it happens to be the author of the blog post -- but here it's from a citation of another paper.) The author is listed, but our similarity search didn't find it. So I don't have a lot of hope that it'll answer our question, but it will nevertheless be instructive to see how it goes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c7766-caf6-458f-929c-5a12a4094d97",
   "metadata": {},
   "source": [
    "Just to confirm, here's the split that includes the author's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffdfaec-715f-46f4-abd3-126e393d7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9a8ca-2fc5-4729-a5fa-f0e2609ebc80",
   "metadata": {},
   "source": [
    "#### Generation (Question Answering) Task\n",
    "We also need to define a function to construct our prompt, send it to the chat model, and handle its response: \n",
    "  1. Knowing from our exploration that the documents returned from the `retrieve` task are objects with different properties, we first need to extract the `page_content` from each. We'll join them into a single string with a couple of line breaks (`\\n\\n`) separating them.\n",
    "  2. Next, we can use the `prompt` template we created earlier, passing to it the question and the chunks we retrieved and processed.\n",
    "  3. Then we `invoke` our chat model, passing to it the `message` (which we recall is a prompt that contains a kind of system message, the source texts, and our question).\n",
    "  4. The `response` we get back from the chat model has a `content` property. That's what we'll return as the `answer`. (If you're curious, you can print out the full response to see what other details you get.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c09ca69-9412-489e-ba25-27da02dbe4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    print(response)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe1f81-125e-4d8d-967b-8be4fb44c28c",
   "metadata": {},
   "source": [
    "Let's give it a try using our example question and the documents we retrieved based on that question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baddcba-f263-45e8-8f34-278deaa232e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(test_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e088c-f886-4092-9243-71765a6f3045",
   "metadata": {},
   "source": [
    "As an experiment, let's see what happens if I add what I know is the relevant information to the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cb102-bd0f-4e97-9a31-859c6e17e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state[\"context\"].append(all_splits[0])\n",
    "generate(test_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac26d7f-62bf-4977-8032-f7814a6b603b",
   "metadata": {},
   "source": [
    "Sweet! That time, we got it. It would be worth experimenting to see if we can tweak our parameters or text-splitting strategy to improve our results. The tool is no good if we have to find the answer and pass it to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c6e97b-68c5-4802-9c9a-b4779f5126df",
   "metadata": {},
   "source": [
    "#### String the Tasks Together\n",
    "Finally, let's add our task definitions to an instance of `LangGraph` and \"compile\" it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01108bd-e836-45f9-92c6-2e4e51f6232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05038419-c953-4020-8ada-20d92ee20d8d",
   "metadata": {},
   "source": [
    "Now, instead of running each task ourselves, we should be able to `invoke` the graph with a question and get an answer. Let's try it with a new question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a05f5-5f45-4857-8398-5c7c038c9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f864b-289e-4fe3-8d83-31620aa2df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = graph.invoke({\"question\": \"You said decomposition breaks large tasks into manageable ones. What makes a task more manageable?\" })\n",
    "print(response2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037db24d-221f-4361-801b-23df24872f8d",
   "metadata": {},
   "source": [
    "## Evaluation, Next Steps\n",
    "What do you think of LangChain? Hard to say since you don't have much experience with it and (probably) even less with other libraries that try to do the same. That's the position you often find yourself in: do I commit to spending more time learning this library? Move on to another??\n",
    "\n",
    "We can also ask how we'd evaluate our application. A couple of things to consider:\n",
    "  - The application has many parts. How can we judge (and possibly improve) each separately? Once we've chained everything together with `LangGraph`, it's really hard to tell where things are failing or not working well. For example, if we'd just asked about the author without looking separately at the retrieval and generation tasks, we wouldn't know that it fails because we just don't retrieve the right information.\n",
    "     - We also have to consider that later steps depend on earlier steps. For example, if retrieval isn't working, maybe we need to tweak how we're chunking the text.\n",
    "  - we relied on a particular embedding and chat model. Would others do better? Worse? What counts as better?\n",
    "\n",
    "We should also take stock of what this application **doesn't** do. For example, each question we ask will be isolated. `reponse2` shows that we haven't built an application capable of holding a conversation where previous conversational turns influence the next response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
